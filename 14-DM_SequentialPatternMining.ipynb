{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Pattern Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import re\n",
    "import textdistance\n",
    "import nltk\n",
    "import ast\n",
    "\n",
    "from collections import OrderedDict\n",
    "from sklearn import preprocessing\n",
    "from numpy import nan\n",
    "from random import randint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from data.tgsp import *\n",
    "from data.spmf import Spmf\n",
    "from prefixspan import PrefixSpan\n",
    "from gsppy.gsp import GSP\n",
    "from apyori import apriori\n",
    "from sklearn.cluster import AgglomerativeClustering \n",
    "from tqdm import tqdm\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/clean_df.csv\", sep='\\t', decimal=',')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Products Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = []\n",
    "products = df['ProductDescription'].unique()\n",
    "print('Total products [{}]'.format(len(products)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalization of products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    #Lower-case\n",
    "    text = str(text)\n",
    "    text = re.sub('[^a-z0-9]+', ' ', text.lower())\n",
    "    text = re.sub(\"\\s\\s+\" , \" \", text)\n",
    "\n",
    "    #Removing numbers\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "  \n",
    "    #NLTK StopWords removal\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "    text = (\" \").join(tokens_without_sw)\n",
    "\n",
    "    #Remove Non-Nouns according to POS tagging\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    text = ''\n",
    "    for t in tagged:\n",
    "        if(t[1].startswith('N')):\n",
    "            text+=' {}'.format(t[0])\n",
    "\n",
    "    #Remove color-material useless infos\n",
    "    BANS = ['red','blue','green','metal','pink','silver','yellow','white','orange']\n",
    "    rpl = tuple([(b,'') for b in BANS])\n",
    "  \n",
    "    text = reduce(lambda a, kv: a.replace(*kv), rpl, text)\n",
    "    return text.strip()\n",
    "\n",
    "def group_texts(texts, threshold=0.5): \n",
    "    #Replace each text with the centroid of each cluster\n",
    "    normalized_texts = np.array([normalize(text) for text in texts])\n",
    "    len_list = len(normalized_texts)\n",
    "    print('[DEBUG] Normalization Done')\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    for i in tqdm(range(len_list)):\n",
    "        row = []\n",
    "        for j in range(len_list):\n",
    "            row.append(1-textdistance.jaro_winkler(normalized_texts[i], normalized_texts[j]))\n",
    "        distances.append(row)\n",
    "    distances = np.array(distances)\n",
    "\n",
    "    clustering = AgglomerativeClustering(\n",
    "        distance_threshold=threshold, # this parameter needs to be tuned carefully\n",
    "        affinity=\"precomputed\", linkage=\"complete\", n_clusters=None\n",
    "    ).fit(distances)\n",
    "  \n",
    "    centers = dict()\n",
    "    for cluster_id in set(clustering.labels_):\n",
    "        index = clustering.labels_ == cluster_id\n",
    "        centrality = distances[:, index][index].sum(axis=1)\n",
    "        centers[cluster_id] = normalized_texts[index][centrality.argmin()]\n",
    "\n",
    "    return [centers[i] for i in clustering.labels_]\n",
    "\n",
    "reduced_prod = group_texts(products)\n",
    "\n",
    "print('------------Starting Products [{}]------------\\n\\n{}\\n'.format(len(products), products))\n",
    "\n",
    "print('------------Clustered Products [{}]------------\\n\\n{}'.format(len(set(reduced_prod)), reduced_prod))\n",
    "\n",
    "print('\\nProducts reduced by {}%'.format(100*len(products)/len(set(reduced_prod))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ProductDescription'].replace(products, reduced_prod,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform Date in number of day from 2010 to 2011 and create list of Carts for each Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['ProductDescription'].str.contains('|'.join(['nan']))]\n",
    "df['CartDate'] = df['CartDate'].apply(lambda r: pd.to_datetime(r).dayofyear + (pd.to_datetime(r).year==2011)*365)\n",
    "\n",
    "groups = df.groupby(['CustomerID','CartDate'])['ProductDescription'].apply(list).reset_index(name='Products')\n",
    "groups = groups.groupby('CustomerID')['Products'].apply(list).reset_index(name='Itemset')\n",
    "groups['Carts'] = groups.apply(lambda rec: len(rec['Itemset']), axis=1)\n",
    "\n",
    "groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = df.copy()\n",
    "timestamps = timestamps.groupby(['CustomerID'])['CartDate'].apply(lambda t: sorted(list(set(t)))).reset_index(name='DayList')\n",
    "timestamps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviour = pd.read_csv(\"data/customer_behaviour_class.csv\", sep='\\t', decimal=',')\n",
    "behaviour = behaviour[['CustomerID', 'CustomerType']]\n",
    "behaviour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = timestamps.merge(groups,on='CustomerID')\n",
    "merged_df = merged_df.merge(behaviour,on='CustomerID')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemset, tmp = {}, {}\n",
    "\n",
    "merged_df = merged_df[merged_df['Carts'] > 1]\n",
    "\n",
    "itemset.update({'HIGH': merged_df[merged_df['CustomerType'] == 'high-spending']['Itemset'].tolist()})\n",
    "itemset.update({'MEDIUM': merged_df[merged_df['CustomerType'] == 'medium-spending']['Itemset'].tolist()})\n",
    "itemset.update({'LOW': merged_df[merged_df['CustomerType'] == 'low-spending']['Itemset'].tolist()})\n",
    "tmp.update({'HIGH': merged_df[merged_df['CustomerType'] == 'high-spending']['DayList'].tolist()})\n",
    "tmp.update({'MEDIUM': merged_df[merged_df['CustomerType'] == 'medium-spending']['DayList'].tolist()})\n",
    "tmp.update({'LOW': merged_df[merged_df['CustomerType'] == 'low-spending']['DayList'].tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('data/[MERGED]SequenceDataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TGSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles, vals = ['LOW','MEDIUM','HIGH'], [80,600,250]\n",
    "\n",
    "for p in zip(profiles, vals):\n",
    "    print('\\n\\n[{}] Profile \\t [{}] Records\\n'.format(p[0], len(itemset[p[0]])))\n",
    "    result_set, rules, freq = apriori(itemset[p[0]],tmp[p[0]], minSupport = p[1], minGap = 1, maxGap = 7, minInterval = 1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PrefixSpan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/[MERGED]SequenceDataset.csv\", sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Itemset to list of list\n",
    "# tmp is a list strings (but these strings are list)\n",
    "tmp = df['Itemset'].values.astype(list).tolist()\n",
    "li = []\n",
    "for i in tmp:\n",
    "    # Convert strings in lists\n",
    "    li.append(ast.literal_eval(i))\n",
    "products = [item for sublist in li for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take unique products for each cart\n",
    "products = [list(set(x)) for x in products]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PrefixSpan(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pattern in at least 10% of Carts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_10 = ps.frequent(1500)\n",
    "#filter=lambda patt, matches: len(patt)>1)\n",
    "pattern_10.sort(key=lambda li: li[0], reverse=True)\n",
    "print(pattern_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_tup = [(l[0], '\\n'.join(l[1])) for l in pattern_10[:20]]\n",
    "pat_tup\n",
    "freq = [t[0] for t in pat_tup]\n",
    "prod = [t[1] for t in pat_tup]\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(prod, freq)\n",
    "plt.ylabel('Frequence of Pattern')\n",
    "plt.yticks(range(0, len(products), 1500))\n",
    "plt.xticks(prod, rotation=45, ha='right')\n",
    "plt.xlabel('Product Description')\n",
    "plt.title('Top 20 of 10% Frequent patterns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSPPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsp_res = GSP(products).search(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsp_res = list(gsp_res[0].items())\n",
    "gsp_tup = []\n",
    "for tup in gsp_res:\n",
    "    gsp_tup.append((tup[1], '\\n'.join(tup[0])))\n",
    "gsp_tup.sort(key=lambda li: li[0], reverse=True)\n",
    "gsp_tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = [t[0] for t in gsp_tup[:20]]\n",
    "prod = [t[1] for t in gsp_tup[:20]]\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(prod, freq)\n",
    "plt.ylabel('Frequence of Pattern')\n",
    "plt.yticks(range(0, len(products), 500))\n",
    "plt.xticks(prod, rotation=45, ha='right')\n",
    "plt.xlabel('Product Description')\n",
    "plt.title('Top 20 of 10% Frequent patterns GSPPy')\n",
    "#plt.savefig('gsppy.png', bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apyori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori_res = apriori(products, min_support=0.1, min_confidence=0.5)\n",
    "apriori_res = list(apriori_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print rules\n",
    "for item in apriori_res:\n",
    "    # item[2] contains all possible rules for an itemset\n",
    "    stats = item[2]\n",
    "    # for each single rule\n",
    "    for rule in stats:\n",
    "        pre = [x for x in rule[0]]\n",
    "        post = [x for x in rule[1]]\n",
    "        pre = ', '.join(pre)\n",
    "        post = ', '.join(post)\n",
    "        print(\"Rule: \" + pre + \" --> \" + post)\n",
    "        print(\"Confidence: \"+ str(rule[2]))\n",
    "        print(\"------------------------------------\")\n",
    "\n",
    "    #Support for an itemset\n",
    "    print(\"Support: \" + str(item[1]))\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apyori_triples = []\n",
    "# Print rules\n",
    "for item in apriori_res:\n",
    "    # item[2] contains all possible rules for an itemset\n",
    "    stats = item[2]\n",
    "    # for each single rule\n",
    "    for rule in stats:\n",
    "        pre = [x for x in rule[0]]\n",
    "        post = [x for x in rule[1]]\n",
    "        pre = ', '.join(pre)\n",
    "        post = ', '.join(post)\n",
    "        rule_str = pre + \" --> \" + post\n",
    "        apyori_triples.append((item[1], rule_str, rule[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apyori_triples.sort(key=lambda li: (li[0], li[2]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support, Rule, Confidence\n",
    "apyori_triples[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input for SPMF package. SPMF takes list of list of list.\n",
    "# The reduced dataframe has form [\"[[s1, s2][s3, s4]]\", \"[[s5, s6]]\"]\n",
    "# Convert reduced dataframe to list of list of list\n",
    "tmp = df['Itemset'].values.astype(list).tolist()\n",
    "spmf_list = []\n",
    "for i in tmp:\n",
    "    # Convert strings in lists\n",
    "    spmf_list.append(ast.literal_eval(i))\n",
    "# Now spmf_list is a list of list of list.\n",
    "\n",
    "# Encode products in number, because SPMF works on number\n",
    "# Take list of all products by open\n",
    "all_prod = [x for subsub in spmf_list for sub in subsub for x in sub]\n",
    "# Products encoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(all_prod)\n",
    "\n",
    "# Remove duplicated products in each Cart and encode product description.\n",
    "spmf_list = [[list(set(le.transform(x))) for x in carts] for carts in spmf_list]\n",
    "# Sort encoded products\n",
    "spmf_list = [[sorted(subsub) for subsub in sub] for sub in spmf_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Customer: \"+ str(len(spmf_list)))\n",
    "n_carts = 0\n",
    "n_customer = 0\n",
    "n_prod = 0\n",
    "n_cart = 0\n",
    "for customer in spmf_list:\n",
    "    n_carts += len(customer)\n",
    "    n_customer +=1\n",
    "    for cart in customer:\n",
    "        n_prod += len(cart)\n",
    "        n_cart += 1\n",
    "print(\"Average Carts per Customer: \"+ str(n_carts/n_customer))\n",
    "print(\"Average Products per Cart: \"+ str(n_prod/n_cart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert encoded number in string.\n",
    "# pattern column is in the form ['n1 n2 n3']. First split by space to obtain list of strings. Then cast str in \n",
    "# int using list(map()) function. At the end use label encoder to retrieve original product.\n",
    "def enc2str(df, le):\n",
    "    return df['pattern'].apply(lambda li: le.inverse_transform(list(map(int, li[0].split(' ')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top-n pattern from SPMF dataframe.\n",
    "# Inputs: spmf datframe, label encoder, number of element to see in plot, percentage of spmf,\n",
    "# algorithm used in spmf, total size of dataset passed to spmf.\n",
    "def plotSpmf(df, le, n=20, percentage=10, algorithm='', total_size=2000):\n",
    "    df['pattern'] = enc2str(df, le)\n",
    "    # Create a list of tuples (products, support)\n",
    "    tmp = [('\\n'.join(list(x)), y)  for x, y in zip(df['pattern'], df['sup'])]\n",
    "    tmp.sort(key=lambda li: li[1], reverse=True)\n",
    "    # Prepare plot\n",
    "    freq = [t[1] for t in tmp[:n]]\n",
    "    prod = [t[0] for t in tmp[:n]]\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.bar(prod, freq)\n",
    "    plt.ylabel('Frequence of Pattern')\n",
    "    plt.yticks(range(0, total_size, 500))\n",
    "    plt.xticks(prod, rotation=45, ha='right')\n",
    "    plt.xlabel('Product Description')\n",
    "    plt.title('Top '+str(n)+' of '+ str(percentage)+'% Frequent patterns SPMF '+algorithm)\n",
    "    #plt.savefig('spmf_'+algorithm+'.png', bbox_inches = \"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments = min support, max length, verbose\n",
    "spmf = Spmf(\"PrefixSpan\", input_direct=spmf_list, spmf_bin_location_dir='./data', arguments=[0.1])\n",
    "spmf.run()\n",
    "patt_PS = spmf.to_pandas_dataframe()\n",
    "plotSpmf(patt_PS, le, n=20, percentage=10, algorithm='PrefixSpan', total_size=len(spmf_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spmf = Spmf(\"GSP\", input_direct=spmf_list, spmf_bin_location_dir='./data', arguments=[0.1])\n",
    "spmf.run()\n",
    "patt_GSP = spmf.to_pandas_dataframe()\n",
    "plotSpmf(patt_GSP, le, n=20, percentage=10, algorithm='GSP', total_size=len(spmf_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spmf = Spmf(\"SPADE\", input_direct=spmf_list, spmf_bin_location_dir='./data', arguments=[0.1])\n",
    "spmf.run()\n",
    "patt_SPD = spmf.to_pandas_dataframe()\n",
    "plotSpmf(patt_SPD, le, n=20, percentage=10, algorithm='SPADE', total_size=len(spmf_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arguments = min support, min pattern size, max pattern size, max gap\n",
    "spmf = Spmf(\"SPAM\", input_direct=spmf_list, spmf_bin_location_dir='./data', arguments=[0.1])\n",
    "spmf.run()\n",
    "patt_SPM = spmf.to_pandas_dataframe()\n",
    "plotSpmf(patt_SPM, le, n=20, percentage=10, algorithm='SPAM', total_size=len(spmf_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spmf = Spmf(\"SPAM\", input_direct=spmf_list, spmf_bin_location_dir='./data', arguments=[0.1, 2])\n",
    "spmf.run()\n",
    "patt_SPM2 = spmf.to_pandas_dataframe()\n",
    "plotSpmf(patt_SPM2, le, n=20, percentage=10, algorithm='SPAM (patt_len>=2)', total_size=len(spmf_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spmf = Spmf(\"LAPIN\", input_direct=spmf_list, spmf_bin_location_dir='./data', arguments=[0.1])\n",
    "spmf.run()\n",
    "patt_LPN = spmf.to_pandas_dataframe()\n",
    "plotSpmf(patt_LPN, le, n=20, percentage=10, algorithm='LAPIN', total_size=len(spmf_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
